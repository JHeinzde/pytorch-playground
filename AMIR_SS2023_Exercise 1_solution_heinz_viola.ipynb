{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Retrieval\n",
    "**Total Points: 7**\n",
    "\n",
    "In this exercise, we will have a look at some essential components of text retrieval. There are many python libraries developed to deal with text data. We will use, scikit-learn, NLTK and Gensim in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-21T13:01:11.319219Z",
     "end_time": "2023-04-21T13:01:13.008430Z"
    }
   },
   "outputs": [],
   "source": [
    "#Import the necessary packages\n",
    "from time import time\n",
    "import csv\n",
    "\n",
    "import gensim.models\n",
    "import pandas as pd\n",
    "import glob\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import WordPunctTokenizer\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK is one of the most prominent libraries for natural language processing. We need to download some of the static features in order to perform preprocessing on text as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-21T13:01:13.008430Z",
     "end_time": "2023-04-21T13:01:13.248469Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jonathan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jonathan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jonathan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing \n",
    "Text preprocessing is one of the most important steps. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-21T13:01:13.245469Z",
     "end_time": "2023-04-21T13:01:13.261472Z"
    }
   },
   "outputs": [],
   "source": [
    "# functions for preprocessing\n",
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)\n",
    "\n",
    "def remove_stop_words_en(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text\n",
    "\n",
    "def remove_punctuation_en(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data\n",
    "\n",
    "def pos_tag_en(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    tagged = []\n",
    "    for w in tokens:\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "    return tagged\n",
    "\n",
    "def remove_apostrophe_en(data):\n",
    "    return np.char.replace(data, \"'\", \"\")\n",
    "\n",
    "def stemming_en(data):\n",
    "    #stemmer= PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + lemmatizer.lemmatize(w)\n",
    "    return new_text\n",
    "\n",
    "def convert_numbers_en(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text\n",
    "\n",
    "def preprocess_en(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation_en(data) #remove comma seperately\n",
    "    data = remove_apostrophe_en(data)\n",
    "    data = remove_stop_words_en(data)\n",
    "    data = convert_numbers_en(data)\n",
    "    data = stemming_en(data)\n",
    "    data = remove_punctuation_en(data)\n",
    "    data = convert_numbers_en(data)\n",
    "    data = stemming_en(data) #needed again as we need to stem the words\n",
    "    data = remove_punctuation_en(data) #needed again as num2word is giving few hypens and commas fourty-one\n",
    "    data = remove_stop_words_en(data) #needed again as num2word is giving stop words 101 - one hundred and one\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-21T13:01:13.263472Z",
     "end_time": "2023-04-21T13:01:13.297471Z"
    }
   },
   "outputs": [],
   "source": [
    "#imports for the bow model\n",
    "from gensim import corpora\n",
    "from gensim import similarities\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will work with the 20-newsgroup dataset. It contains 20000 messages taken from 20 various newsgroups. There are 20 classes such as alt.atheism, comp.graphics, comp.os.ms-windows.misc, comp.sys.ibm.pc.hardware, comp.sys.mac.hardware. We will try to use simple bag of words model to retieve data related to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-21T13:01:13.280472Z",
     "end_time": "2023-04-21T13:01:13.481470Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "\n",
    "news_group = fetch_20newsgroups(subset='train')\n",
    "\n",
    "news_group_data = news_group.data\n",
    "news_group_target_names = news_group.target_names\n",
    "news_group_target = news_group.target\n",
    "\n",
    "# Creating a dataframe from the loaded data\n",
    "news_df = pd.DataFrame({'news': news_group_data, \n",
    "                        'class': news_group_target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-21T13:01:13.482470Z",
     "end_time": "2023-04-21T13:01:13.497471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(list(news_group.target_names)) #class names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-21T13:01:13.499473Z",
     "end_time": "2023-04-21T13:02:49.709648Z"
    }
   },
   "outputs": [],
   "source": [
    "textDF1 = news_df.to_dict()\n",
    "#preprocess data inorder to create the  dictionary and indexes\n",
    "article_text = []\n",
    "for x in range(len(textDF1['news'])):\n",
    "    article_text.append(word_tokenize(str(preprocess_en(textDF1['news'][x]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index creation \n",
    "In index creation, we index documents in order to prepare them for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-21T13:02:49.711213Z",
     "end_time": "2023-04-21T13:02:52.825184Z"
    }
   },
   "outputs": [],
   "source": [
    "#build dictionary for corpus and get the similarity matrix for the dictionary\n",
    "corpora_dict = corpora.Dictionary(article_text)\n",
    "for token, token_id in corpora_dict.token2id.items():\n",
    "    corpora_dict.id2token[token_id] = token\n",
    "corpus = [corpora_dict.doc2bow(text) for text in article_text]\n",
    "index_bow = similarities.SparseMatrixSimilarity(corpus, num_features=len(corpora_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-21T13:02:52.826183Z",
     "end_time": "2023-04-21T13:02:52.841185Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "113605"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpora_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise -- 1 point**\n",
    "\n",
    "Play around with the preprocessing techniques and observe the change in the length of the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Solution Exercise 1\n",
    "\n",
    "The first thing I did was comment out everything in the preprocessing function aside from the first line to see what the difference there is:\n",
    "\n",
    "```python\n",
    "def preprocess_en(data):\n",
    "    data = convert_lower_case(data)\n",
    "    #data = remove_punctuation_en(data) #remove comma seperately\n",
    "    #data = remove_apostrophe_en(data)\n",
    "    #data = remove_stop_words_en(data)\n",
    "    #data = convert_numbers_en(data)\n",
    "    #data = stemming_en(data)\n",
    "    #data = remove_punctuation_en(data)\n",
    "    #data = convert_numbers_en(data)\n",
    "    #data = stemming_en(data) #needed again as we need to stem the words\n",
    "    #data = remove_punctuation_en(data) #needed again as num2word is giving few hypens and commas fourty-one\n",
    "    #data = remove_stop_words_en(data) #needed again as num2word is giving stop words 101 - one hundred and one\n",
    "    return data\n",
    "```\n",
    "\n",
    "The result of this was a stark increase of corpus size from 113605 to 181531.\n",
    "Next step I iteratively uncommented the next line and will not the corpus sizes resulting below:\n",
    "\n",
    " 1. 131458\n",
    "2. 131335\n",
    "3. 131148\n",
    "4. 120347\n",
    "5. 113727\n",
    "6. 113726\n",
    "7. 113692\n",
    "8. 113666\n",
    "9. 113666\n",
    "10. 113605\n",
    "\n",
    "\n",
    "Interestingly enough we can see clearly that each of the processing step decreases the corpus size, while between 8 and 9 there seems to be no such decrease."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Exercise -- 2 points**\n",
    "\n",
    "Create a tfidf index using [tfidf](https://radimrehurek.com/gensim/models/tfidfmodel.html) model from gesim."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Solution Exercise 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "model = models.TfidfModel(corpus)\n",
    "index_tfidf = similarities.SparseMatrixSimilarity(model[corpus], num_features=len(corpora_dict))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-21T13:02:52.842184Z",
     "end_time": "2023-04-21T13:02:55.470520Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query search\n",
    "Once the documents are indexed, the corpus can be used for querying. For a given query, we find the cosine similarities between query and all documents. The documents with highest similarity values, are treated as matches and are retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-21T13:02:55.473806Z",
     "end_time": "2023-04-21T13:02:55.485522Z"
    }
   },
   "outputs": [],
   "source": [
    "#main search function.\n",
    "def search(index, query):\n",
    "    bow_vec = corpora_dict.doc2bow(query.lower().split()) \n",
    "    similarities = index[bow_vec]  # get cosine similarities between the query and all index documents\n",
    "    similarities = [(x, i) for i, x in enumerate(similarities)]\n",
    "    similarities.sort(key=lambda elem: -elem[0])# sorting by similarity_value in decreasing order\n",
    "    conf = []\n",
    "    classes = list(news_df['class'])\n",
    "    file_id = list(textDF1['class'])\n",
    "    for i in range(len(similarities)):\n",
    "        try:\n",
    "            if (similarities[i][0] > 0.05): #change the confidence value here if you wish too\n",
    "                conf.append(similarities[i])\n",
    "        except IndexError:\n",
    "            IndexError\n",
    "    #print(len(conf))\n",
    "    res = []\n",
    "    for result in conf:\n",
    "        res.append((file_id[result[1]],classes[result[1]],result[0], article_text[file_id[result[1]]]))\n",
    "    return res[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise -- 4 points**\n",
    "\n",
    "Search for the following queries using BOW and TFIDF index. Compare top 5 results and comment on the differences. \n",
    "1. hardware\n",
    "2. space\n",
    "3. natural\n",
    "4. sport\n",
    "5. medicine"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Solution Exercise 4 "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-21T13:38:35.221691Z",
     "end_time": "2023-04-21T13:38:35.463692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------SEARCH WORD hardware-------------------\n",
      "\n",
      "-------------------RANK 0-------------------\n",
      "bow: 3908\n",
      "tfidf 3908\n",
      "-------------------RANK 1-------------------\n",
      "bow: 8257\n",
      "tfidf 8257\n",
      "-------------------RANK 2-------------------\n",
      "bow: 11205\n",
      "tfidf 802\n",
      "-------------------RANK 3-------------------\n",
      "bow: 802\n",
      "tfidf 11205\n",
      "-------------------RANK 4-------------------\n",
      "bow: 10435\n",
      "tfidf 10752\n",
      "\n",
      "\n",
      "\n",
      "-------------------SEARCH WORD space-------------------\n",
      "\n",
      "-------------------RANK 0-------------------\n",
      "bow: 7545\n",
      "tfidf 9986\n",
      "-------------------RANK 1-------------------\n",
      "bow: 2800\n",
      "tfidf 7545\n",
      "-------------------RANK 2-------------------\n",
      "bow: 4504\n",
      "tfidf 2800\n",
      "-------------------RANK 3-------------------\n",
      "bow: 1665\n",
      "tfidf 4425\n",
      "-------------------RANK 4-------------------\n",
      "bow: 6707\n",
      "tfidf 4504\n",
      "\n",
      "\n",
      "\n",
      "-------------------SEARCH WORD natural-------------------\n",
      "\n",
      "-------------------RANK 0-------------------\n",
      "bow: 5800\n",
      "tfidf 5800\n",
      "-------------------RANK 1-------------------\n",
      "bow: 2509\n",
      "tfidf 2509\n",
      "-------------------RANK 2-------------------\n",
      "bow: 480\n",
      "tfidf 1810\n",
      "-------------------RANK 3-------------------\n",
      "bow: 1810\n",
      "tfidf 480\n",
      "-------------------RANK 4-------------------\n",
      "bow: 9766\n",
      "tfidf 9766\n",
      "\n",
      "\n",
      "\n",
      "-------------------SEARCH WORD sport-------------------\n",
      "\n",
      "-------------------RANK 0-------------------\n",
      "bow: 6928\n",
      "tfidf 6928\n",
      "-------------------RANK 1-------------------\n",
      "bow: 1847\n",
      "tfidf 1847\n",
      "-------------------RANK 2-------------------\n",
      "bow: 4864\n",
      "tfidf 4985\n",
      "-------------------RANK 3-------------------\n",
      "bow: 4985\n",
      "tfidf 1153\n",
      "-------------------RANK 4-------------------\n",
      "bow: 1153\n",
      "tfidf 4864\n",
      "\n",
      "\n",
      "\n",
      "-------------------SEARCH WORD medicine-------------------\n",
      "\n",
      "-------------------RANK 0-------------------\n",
      "bow: 2329\n",
      "tfidf 2329\n",
      "-------------------RANK 1-------------------\n",
      "bow: 6793\n",
      "tfidf 6033\n",
      "-------------------RANK 2-------------------\n",
      "bow: 6033\n",
      "tfidf 6793\n",
      "-------------------RANK 3-------------------\n",
      "bow: 8034\n",
      "tfidf 8034\n",
      "-------------------RANK 4-------------------\n",
      "bow: 1074\n",
      "tfidf 1074\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_words = [\"hardware\", \"space\", \"natural\", \"sport\", \"medicine\"]\n",
    "\n",
    "for s_word in search_words:\n",
    "    one_tfidf = search(index_tfidf, s_word)\n",
    "    one_bow = search(index_bow, s_word)\n",
    "    print(f\"-------------------SEARCH WORD {s_word}-------------------\")\n",
    "    print()\n",
    "    for i in range(5):\n",
    "        print(f'-------------------RANK {i}-------------------')\n",
    "        print('bow:', one_bow[i][0])\n",
    "        print('tfidf', one_tfidf[i][0])\n",
    "    print()\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generally we can observe that both indicies find very similar things, but sometimes they are ranked differently.\n",
    "\n",
    "For example in the last case (5) we can see that they find the same documents, but ranks 1 and 2 are switched between bow and tfidf indicies.\n",
    "We can observe similar things in case 1 where only the last item differs completely (tfidf finds a different document). This is probably due to the fact that the TFIDF Index values repeated occurrences of a word lower than the BoW (Bag of Words Model does). One example would be the documents 6033 and 6793 in case 5. I printed them out below. We can observe that the document 6793 has more occurrences of the word \"medicine\" in it than the document 6033 but the tfidf Index ranks 6033 higher than 6793. This effect can not only be observed in this case but other cases for example when searching for \"natural\" the documents 480 and 1810. What can also be observed that BoW tends to rank longer texts higher than TFIDF (which is in line with our previous discovery as longer text probably also have more occurrences of the same word."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: kxgst1+@pitt.edu (Kenneth Gilbert)\n",
      "Subject: Re: quality control in medicine\n",
      "Organization: University of Pittsburgh\n",
      "Lines: 20\n",
      "\n",
      "In article <93108.003258U19250@uicvm.uic.edu> U19250@uicvm.uic.edu writes:\n",
      ":Does anybody know of any information regarding the implementaion of total\n",
      ": quality management, quality control, quality assurance in the delivery of\n",
      ": health care service.  I would appreciate any information.  If there is enough\n",
      ":interest, I will post the responses.\n",
      "\n",
      "\n",
      "This is in fact a hot topic in medicine these days, and much of the\n",
      "medical literature is devoted to this.  The most heavily funded studies\n",
      "these days are for outcome research, and physicians (and others!) are\n",
      "constantly questionning whether what we do it truly effective in any given\n",
      "situation.  QA activities are a routine part of every hospital's\n",
      "administrative function and are required by accreditation agencies.  There\n",
      "are even entire publications devoted to QA issues.\n",
      "\n",
      "-- \n",
      "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-|-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "=  Kenneth Gilbert              __|__        University of Pittsburgh   =\n",
      "=  General Internal Medicine      |      \"...dammit, not a programmer!\" =\n",
      "=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-|-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "\n",
      "From: rgasch@nl.oracle.com (Robert Gasch)\n",
      "Subject: Re: Homeopathy: a respectable medical tradition?\n",
      "Organization: Oracle Europe\n",
      "Lines: 47\n",
      "X-Newsreader: TIN [version 1.1 PL8]\n",
      "\n",
      "Gordon Banks (geb@cs.pitt.edu) wrote:\n",
      ": In article <3794@nlsun1.oracle.nl> rgasch@nl.oracle.com (Robert Gasch) writes:\n",
      ": >\n",
      ": >: From a business point of view, it might make sense.  It depends on\n",
      ": >: the personality of the practitioner.  If he can charm the patients\n",
      ": >: into coming, homeopathy can be very profitable.  It won't be covered\n",
      ": >: by insurance, however.  Just keep that in mind.  Myself, I'd have \n",
      ": >^^^^^^^^^^^^^^^^^^^^^^^\n",
      ": >\n",
      ": >In many European countries Homepathy is accepted as a method of curing\n",
      ": >(or at least alleiating) many conditions to which modern medicine has \n",
      ": >no answer. In most of these countries insurance pays for the \n",
      ": >treatments.\n",
      ": >\n",
      "\n",
      ": Accepted by whom?  Not by scientists.  There are people\n",
      ": in every country who waste time and money on quackery.\n",
      ": In Britain and Scandanavia, where I have worked, it was not paid for.\n",
      ": What are \"most of these countries?\"  I don't believe you.\n",
      "\n",
      "In Holland insurences pay for Homeopathic treatment. In Germany they do\n",
      "so as well. I Austria they do if you have a condition which can not be \n",
      "helped by \"normal\" medicine (happened to me). Switzerland seems to be \n",
      "the same as Austria (I have direct experience in the Swiss case).\n",
      "\n",
      "At the Univeristy of Vienna (I believe Innsbruck as well) homeopathy\n",
      "can be taken in Med. school.\n",
      "\n",
      "I found that in combination with Acupuncture it changed my life from\n",
      "living hell to a condition which enables me to lead a relatively \n",
      "normal life. I found that modern medicine was powerless to cure me\n",
      "of a *severe* case of Neurodermitis (Note: I mean cure, not \n",
      "surpress the symptoms, which is what modern medicine attempts to \n",
      "do in the case of Neurodermitis). \n",
      "\n",
      "I'm not saying that Homeopathy is scientific, but that it can offer \n",
      "help in areas in which modern medicine is absolutely helpless.\n",
      "\n",
      "From reading your aritcle it seems that your have some deeply rooted\n",
      "beliefs about this issue (this is not intended to be offensive or \n",
      "sarcastic - it just sounded like that to me) which makes me doubt \n",
      "if you can read this with an open mind. If you do/can, please excuse\n",
      "my last comment.\n",
      "\n",
      "---> Robert\n",
      "rgasch@nl.oracle.com\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(textDF1['news'][6033])\n",
    "print(\"BORDER\")\n",
    "print(textDF1['news'][6793])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-21T13:07:51.594744Z",
     "end_time": "2023-04-21T13:07:51.623742Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: keith@cco.caltech.edu (Keith Allan Schneider)\n",
      "Subject: Re: <Political Atheists?\n",
      "Organization: California Institute of Technology, Pasadena\n",
      "Lines: 20\n",
      "NNTP-Posting-Host: punisher.caltech.edu\n",
      "\n",
      "livesey@solntze.wpd.sgi.com (Jon Livesey) writes:\n",
      "\n",
      ">Perhaps the chimps that failed to evolve cooperative behaviour\n",
      ">died out, and we are left with the ones that did evolve such\n",
      ">behaviour, entirely by chance.\n",
      "\n",
      "That's the entire point!\n",
      "\n",
      ">Are you going to proclaim a natural morality every time an\n",
      ">organism evolves cooperative behaviour?\n",
      "\n",
      "Yes!\n",
      "\n",
      "Natural morality is a morality that developed naturally.\n",
      "\n",
      ">What about the natural morality of bee dance?\n",
      "\n",
      "Huh?\n",
      "\n",
      "keith\n",
      "\n",
      "BORDER\n",
      "From: kcochran@nyx.cs.du.edu (Keith \"Justified And Ancient\" Cochran)\n",
      "Subject: We don't need no stinking subjects!\n",
      "X-Disclaimer: Nyx is a public access Unix system run by the University\n",
      "\tof Denver for the Denver community.  The University has neither\n",
      "\tcontrol over nor responsibility for the opinions of users.\n",
      "Organization: The Loyal Order Of Keiths.\n",
      "Lines: 93\n",
      "\n",
      "In article <1ql1avINN38a@gap.caltech.edu> keith@cco.caltech.edu (Keith Allan Schneider) writes:\n",
      ">kcochran@nyx.cs.du.edu (Keith \"Justified And Ancient\" Cochran) writes:\n",
      ">>keith@cco.caltech.edu (Keith Allan Schneider) writes:\n",
      ">>>kcochran@nyx.cs.du.edu (Keith \"Justified And Ancient\" Cochran) writes:\n",
      ">\n",
      ">>No, if you're going to claim something, then it is up to you to prove it.\n",
      ">>Think \"Cold Fusion\".\n",
      ">\n",
      ">Well, I've provided examples to show that the trend was general, and you\n",
      ">(or others) have provided some counterexamples, mostly ones surrounding\n",
      ">mating practices, etc.  I don't think that these few cases are enough to\n",
      ">disprove the general trend of natural morality.  And, again, the mating\n",
      ">practices need to be reexamined...\n",
      "\n",
      "So what you're saying is that your mind is made up, and you'll just explain\n",
      "away any differences at being statistically insignificant?\n",
      "\n",
      ">>>Try to find \"immoral\" non-mating-related activities.\n",
      ">>So you're excluding mating-related-activities from your \"natural morality\"?\n",
      ">\n",
      ">No, but mating practices are a special case.  I'll have to think about it\n",
      ">some more.\n",
      "\n",
      "So you'll just explain away any inconsistancies in your \"theory\" as being\n",
      "\"a special case\".\n",
      "\n",
      ">>>Yes, I think that the natural system can be objectively deduced with the\n",
      ">>>goal of species propogation in mind.  But, I am not equating the two\n",
      ">>>as you so think.  That is, an objective system isn't necessarily the\n",
      ">>>natural one.\n",
      ">>Are you or are you not the man who wrote:\n",
      ">>\"A natural moral system is the objective moral system that most animals\n",
      ">> follow\".\n",
      ">\n",
      ">Indeed.  But, while the natural system is objective, all objective systems\n",
      ">are not the natural one.  So, the terms can not be equated.  The natural\n",
      ">system is a subset of the objective ones.\n",
      "\n",
      "You just equated them.  Re-read your own words.\n",
      "\n",
      ">>Now, since homosexuality has been observed in most animals (including\n",
      ">>birds and dolphins), are you going to claim that \"most animals\" have\n",
      ">>the capacity of being immoral?\n",
      ">\n",
      ">I don't claim that homosexuality is immoral.  It isn't harmful, although\n",
      ">it isn't helpful either (to the mating process).  And, when you say that\n",
      ">homosexuality is observed in the animal kingdom, don't you mean \"bisexuality?\"\n",
      "\n",
      "A study release in 1991 found that 11% of female seagulls are lesbians.\n",
      "\n",
      ">>>Well, I'm saying that these goals are not inherent.  That is why they must\n",
      ">>>be postulates, because there is not really a way to determine them\n",
      ">>>otherwise (although it could be argued that they arise from the natural\n",
      ">>>goal--but they are somewhat removed).\n",
      ">>Postulate: To assume; posit.\n",
      ">\n",
      ">That's right.  The goals themselves aren't inherent.\n",
      ">\n",
      ">>I can create a theory with a postulate that the Sun revolves around the\n",
      ">>Earth, that the moon is actually made of green cheese, and the stars are\n",
      ">>the portions of Angels that intrudes into three-dimensional reality.\n",
      ">\n",
      ">You could, but such would contradict observations.\n",
      "\n",
      "Now, apply this last sentence of your to YOUR theory.  Notice how your are\n",
      "contridicting observations?\n",
      "\n",
      ">>I can build a mathematical proof with a postulate that given the length\n",
      ">>of one side of a triangle, the length of a second side of the triangle, and\n",
      ">>the degree of angle connecting them, I can determine the length of the\n",
      ">>third side.\n",
      ">\n",
      ">But a postulate is something that is generally (or always) found to be\n",
      ">true.  I don't think your postulate would be valid.\n",
      "\n",
      "You don't know much math, do you?  The ability to use SAS to determine the\n",
      "length of the third side of the triangle is fundemental to geometry.\n",
      "\n",
      ">>Guess which one people are going to be more receptive to.  In order to assume\n",
      ">>something about your system, you have to be able to show that your postulates\n",
      ">>work.\n",
      ">\n",
      ">Yes, and I think the goals of survival and happiness *do* work.  You think\n",
      ">they don't?  Or are they not good goals?\n",
      "\n",
      "Goals <> postulates.\n",
      "\n",
      "Again, if one of the \"goals\" of this \"objective/natural morality\" system\n",
      "you are proposing is \"survival of the species\", then homosexuality is\n",
      "immoral.\n",
      "--\n",
      "=kcochran@nyx.cs.du.edu | B(0-4) c- d- e++ f- g++ k(+) m r(-) s++(+) t | TSAKC=\n",
      "=My thoughts, my posts, my ideas, my responsibility, my beer, my pizza.  OK???=\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(textDF1['news'][480])\n",
    "print(\"BORDER\")\n",
    "print(textDF1['news'][1810])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-21T13:26:16.818653Z",
     "end_time": "2023-04-21T13:26:16.843650Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
